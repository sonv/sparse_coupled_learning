1. How can I validate this output? Also, would you have any other sample problems I can run? This is primarily so that I can include some pretty pictures and examples of problems this framework can solve in my Thesis. I am also open to suggestions about interesting problems I can demonstrate this on.

- train on classification problems
- apply nonlin at the end 

2. Could you elaborate a bit more on what you mentioned last time regarding "interesting problems to study using large linear networks"?

- makes more sense to study what large systems do wrt smaller ones
- train time dir prop to delta bw free and clamped distance (source and target)
- many questions wrt system size
- no square lattices (DONE)
- inc dist bw source and target (DONE)
- something simple but can use scalability of this code
- "width" - add a hidden layer in the middle thats wide - more alternatives from source to nodes
- for chips we dont know sources and targets connectivity
- any intuition is useful
- current vague intution - compared to just barebones, learning will be easier, will improve in some sense as gets wider, diminshing returns
- not only track error, but also how much power the solution takes
- which architectures look interesting
- do not increase in general path lengths between sources and targets (all length 2 for example)
- DP = dot(sDMF, PF), K*DP^2 (is power)
- see how architecture affects power (without biases)

3. I am planning to clean up the Linear Solver class as well. Would you have any suggestions as to what I should add to it?

- add a layer to make it more useful for classification problems (INTERESTING) - can do in multiple ways
- need to see a great way to discretize
- if correct classification do nothing, if wrong, apply learning rule - IDENTICAL TO CROSS ENTROPY

4. I have started writing the introduction to my Thesis. I will keep you posted on how I progress with that.

- make sure to include systems stuff
- make sure to have physics details also


Mar 29, 2024

- find tasks that get zero training error (TODO)
- power (TODO)
- classification
- need to figure out what the label is from output
- take all training data, pass through untrained networks 
- now take all average for all training data associated with class 1
- that average is now the desired started
- sometimes reassiging labels helps
- just get free state values at that node
- we dont want to do mean sq error like usual
- binary cross entropy
- check which label the output is closest to
- if correct classification do nothing, if wrong, apply learning rule
- this is the same as minimizing cross entropy ( see PROOF )

PSEUDOCODE

- get entries from first n entries of PF
- do this kmeans style procedure to get centroids for targets
- classification accuracy begins high but still improves
- use IRIS dataset (more or less linearly separable)

- package name (something)
- documentation etc (TODO)
- rerun width

April 5, 2024

- not expected to be groundbreaking
- just dump everything we need to know
- we made it practically usable
- show few examples
- in the end we can say we used it to do something new

April 12, 2024

- THESE ARE ENOUGH
- the extent of connectivity (connect to the next layer, and some above and below that - else not geometrically neighbours)

- effect of width (1-10 nodes for physics) (width similar to sources and targets)
    done

- effect of topology
    added topology stuff

- scaling behaviour (some interesting behavior in small nets)
    added memory and timing stuff for several sizes

- classification (subset of iris)

- double learning (for any size) - purely physical phenomenon
    for n = 20 nodes
    1 init = 250k iters
    random init = 150k iters

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)